{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 110379,
          "sourceType": "datasetVersion",
          "datasetId": 57322
        },
        {
          "sourceId": 2731709,
          "sourceType": "datasetVersion",
          "datasetId": 1665327
        },
        {
          "sourceId": 4032467,
          "sourceType": "datasetVersion",
          "datasetId": 2389252
        }
      ],
      "dockerImageVersionId": 30042,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jannesklaas_scifi_stories_text_corpus_path = kagglehub.dataset_download('jannesklaas/scifi-stories-text-corpus')\n",
        "nomisethi1_figlang_train_path = kagglehub.dataset_download('nomisethi1/figlang-train')\n",
        "ronikdedhia_next_word_prediction_path = kagglehub.dataset_download('ronikdedhia/next-word-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "73wwE_1g6OK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16de52f-b4cf-4042-83f0-04ef8c697457"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jannesklaas/scifi-stories-text-corpus?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55.7M/55.7M [00:00<00:00, 69.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nomisethi1/figlang-train?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32.6M/32.6M [00:00<00:00, 68.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(jannesklaas_scifi_stories_text_corpus_path)\n",
        "print(nomisethi1_figlang_train_path)\n",
        "print(ronikdedhia_next_word_prediction_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJyKdxTH6s9D",
        "outputId": "e6cd5add-33af-4a3c-fd8c-d8a85171d7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/jannesklaas/scifi-stories-text-corpus/versions/1\n",
            "/root/.cache/kagglehub/datasets/nomisethi1/figlang-train/versions/1\n",
            "/kaggle/input/next-word-prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Content\n",
        "\n",
        "* [üìö Theory](#section-one)\n",
        "    * [The Bigram Model ‚ë°](#bigram-model)\n",
        "    * [Estimation ‚©∞](#estimation)\n",
        "* [üìÇ Basic Setup](#basic-setup)\n",
        "* [üßΩ Pre-Processing pipeline](#pre-process)\n",
        "* [‚úÇÔ∏è Splitting into Train, Valid and Test](#split)\n",
        "* [üßπ Cleaning the Data](#clean)\n",
        "    * [üìî Creating a Frequency Dictionary](#frequency)\n",
        "    * [üîí Creating a Closed Vocabulary](#closed)\n",
        "    * [ü§∑üèª Adding UNK Tokens](#unk)\n",
        "    * [üßº Final Cleaning Pipeline](#final)\n",
        "* [üí™üèª Building The \"Model\"](#build)\n",
        "* [üí¨ The Auto-Complete System](#auto-complete)\n",
        "* [üòä Inference](#inference)\n",
        "* [üßê Miscellaneous](#misc)"
      ],
      "metadata": {
        "id": "d6R7ZSi76OK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-one\"></a>\n",
        "# üìö Theory\n",
        "\n",
        "Let's delve into the theory and try to gain a intuition about n-gram language models.\n",
        "\n",
        "N-Gram models are Statistical(Probabilistic) Language models that aim to assign probabilities to a given sequence of words. Any N-gram is just a sequence of \"n\" words. For example, \"Saurav\" is a unigram and \"Hi There\" is a bigram.\n",
        "\n",
        "\n",
        "The task is to find out if we can compute $P(w | h)$ given a word $w$ and some history $h$. One could say that we can compute the probability of a given next word, using all the previous words in the sentence. For example using the last sentence, we could calculate:\n",
        "\n",
        "$$\\large\n",
        "P ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "One such approach could be to use **relative frequency counts** to compute this probability, i.e. ,**Out of the times we saw the history $h$, how many times was it followed by the word $w$**\n",
        "\n",
        "Or\n",
        "\n",
        "$$\n",
        "P ( \\, word \\, | \\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,) = \\frac{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\, word)}{C(\\, One \\, could \\, say \\, that \\, we \\, can \\, compute \\, the \\, probability \\, of \\, a \\, given \\, next \\,)}\n",
        "$$\n",
        "---\n",
        "\n",
        "Intuitively it seems infeasible to perform this over an entire corpus; especially it is of a significant a size. This is the motivation behind the N-gram model, instead of using the entire corpus, we approximate this probability using just `n` previous words.\n",
        "\n",
        "For instance if $w_{1:n}$ represents the sequence of words $w_1w_2...w_n$, then using the chain rule of probability we can write,  \n",
        "\n",
        "\n",
        "$$\\large\n",
        "P(w_{1:n}) = P(w_1)P(w_2 | w_1)P(w_3 | w_{1:2})...P(w_n|w_{1:n-1})\n",
        "$$\n",
        "\n",
        "\n",
        "$$\\large\n",
        "P(w_{1:n}) = \\prod_{k=1}^{n}P(w_k | w_{1:k-1})\n",
        "$$\n",
        "\n",
        "<a id=\"bigram-model\"></a>\n",
        "## The Bigram Model ‚ë°\n",
        "\n",
        "A Bigram Model corresponds to a model which approximates the probability of a word given all the previous words $P(w_n|w_{1:n‚àí1})$ by using only the conditional probability of the preceding word $P(w_n|w_{n‚àí1})$. Thus we assume that $P(w_n|w_{1:n‚àí1}) ‚âà P(w_n|w_{n‚àí1})$. This approximation is known as the **Markov** approximation. Thus, for the Bigram model, the probability for an entire sequence can be approximated as:\n",
        "\n",
        "$$\\large\n",
        "P(w_{1:n}) ‚âà \\prod_{k=1}^{n}P(w_{k}|w_{k‚àí1})\n",
        "$$\n",
        "\n",
        "<a id=\"estimation\"></a>\n",
        "## Estimation ‚©∞\n",
        "\n",
        "To estimate such probabilities we use the **Maximum Likelihood Estimation (MLE)**. An MLE estimate for the parameters of an n-gram model can be obtained by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\n",
        "\n",
        "For a Bigram model, the MLE Estimation can be given by:\n",
        "\n",
        "$$\\large\n",
        "P(w_n | w_{n-1}) \\frac{C(w_{n-1}w_n)}{\\sum_{w} C(w_{n-1}w)}\n",
        "$$\n",
        "\n",
        "---\n",
        "For the general case of MLE n-gram parameter estimation:\n",
        "\n",
        "$$\\large\n",
        "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n",
        "$$"
      ],
      "metadata": {
        "id": "n2XMLfaP6OK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_path = \"/root/.cache/kagglehub/datasets/nomisethi1/figlang-train/versions/1\"\n",
        "file_name = data_path + \"/train.txt\"\n",
        "\n",
        "nltk.data.path.append(data_path)\n",
        "nltk.download('punkt')\n",
        "\n",
        "with open(file_name, \"r\") as f:\n",
        "    data = f.read()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:24:24.756426Z",
          "iopub.execute_input": "2025-04-29T09:24:24.756838Z",
          "iopub.status.idle": "2025-04-29T09:24:25.632267Z",
          "shell.execute_reply.started": "2025-04-29T09:24:24.7568Z",
          "shell.execute_reply": "2025-04-29T09:24:25.63123Z"
        },
        "id": "yiUiS1TN6OK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71dff883-3eae-4a3d-8a1d-bb2fdab9271f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocessing(input_data) -> 'list':\n",
        "\n",
        "    sentences = input_data.split('\\n')\n",
        "\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "    tokenized = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        token = nltk.word_tokenize(sentence)\n",
        "\n",
        "        tokenized.append(token)\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "tokenized_sentences = preprocessing(data)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:24:25.63456Z",
          "iopub.execute_input": "2025-04-29T09:24:25.635097Z",
          "iopub.status.idle": "2025-04-29T09:26:58.686999Z",
          "shell.execute_reply.started": "2025-04-29T09:24:25.635036Z",
          "shell.execute_reply": "2025-04-29T09:26:58.68587Z"
        },
        "id": "3S_OLHga6OK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5f7724-9d50-4d98-e1be-e4207756b895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "train_set, val_set = train_test_split(train_set, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:26:58.688856Z",
          "iopub.execute_input": "2025-04-29T09:26:58.689193Z",
          "iopub.status.idle": "2025-04-29T09:26:59.064935Z",
          "shell.execute_reply.started": "2025-04-29T09:26:58.68916Z",
          "shell.execute_reply": "2025-04-29T09:26:59.063911Z"
        },
        "id": "OmXh5nDD6OK-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(sentences) -> 'dict':\n",
        "\n",
        "    count = {}\n",
        "    for sentence in sentences:\n",
        "\n",
        "        for token in sentence:\n",
        "            if token not in count.keys():\n",
        "                count[token] = 1\n",
        "\n",
        "            else:\n",
        "                count[token] += 1\n",
        "\n",
        "    return count"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:26:59.066294Z",
          "iopub.execute_input": "2025-04-29T09:26:59.06661Z",
          "iopub.status.idle": "2025-04-29T09:26:59.102637Z",
          "shell.execute_reply.started": "2025-04-29T09:26:59.066575Z",
          "shell.execute_reply": "2025-04-29T09:26:59.101122Z"
        },
        "id": "i9PW-S5u6OK-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_vocab(tokenized_sentences, count_threshold) -> 'list':\n",
        "\n",
        "    closed_vocabulary = []\n",
        "    temp = word_count(tokenized_sentences)\n",
        "\n",
        "    for word, count in temp.items():\n",
        "        if count >= count_threshold :\n",
        "                closed_vocabulary.append(word)\n",
        "\n",
        "    return closed_vocabulary"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:26:59.106726Z",
          "iopub.execute_input": "2025-04-29T09:26:59.107153Z",
          "iopub.status.idle": "2025-04-29T09:26:59.123417Z",
          "shell.execute_reply.started": "2025-04-29T09:26:59.107117Z",
          "shell.execute_reply": "2025-04-29T09:26:59.122121Z"
        },
        "id": "2iSSa2Cq6OK-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def unknown(tokenized_sentences, vocabulary, unknown_token = \"<unknown>\") -> 'list':\n",
        "    vocabulary = set(vocabulary)\n",
        "    new_tokenized_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "\n",
        "        new_sentence = []\n",
        "        for token in sentence:\n",
        "            if token in vocabulary:\n",
        "                new_sentence.append(token)\n",
        "            else:\n",
        "                new_sentence.append(unknown_token)\n",
        "        new_tokenized_sentences.append(new_sentence)\n",
        "    return new_tokenized_sentences"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:26:59.125524Z",
          "iopub.execute_input": "2025-04-29T09:26:59.125982Z",
          "iopub.status.idle": "2025-04-29T09:26:59.145271Z",
          "shell.execute_reply.started": "2025-04-29T09:26:59.125934Z",
          "shell.execute_reply": "2025-04-29T09:26:59.144205Z"
        },
        "id": "Dm9c-V9o6OK_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def cleansing(train_data, test_data, count_threshold):\n",
        "    vocabulary = closed_vocab(train_data, count_threshold)\n",
        "    new_train_data = unknown(train_data, vocabulary)\n",
        "    new_test_data = unknown(test_data, vocabulary)\n",
        "\n",
        "    return new_train_data, new_test_data, vocabulary"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:26:59.146547Z",
          "iopub.execute_input": "2025-04-29T09:26:59.146884Z",
          "iopub.status.idle": "2025-04-29T09:26:59.160482Z",
          "shell.execute_reply.started": "2025-04-29T09:26:59.146852Z",
          "shell.execute_reply": "2025-04-29T09:26:59.159294Z"
        },
        "id": "uLiN9iRK6OK_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "min_freq = 6\n",
        "final_train, final_test, vocabulary = cleansing(train_set, test_set, min_freq)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:26:59.16214Z",
          "iopub.execute_input": "2025-04-29T09:26:59.162586Z",
          "iopub.status.idle": "2025-04-29T09:27:07.876162Z",
          "shell.execute_reply.started": "2025-04-29T09:26:59.16253Z",
          "shell.execute_reply": "2025-04-29T09:27:07.874936Z"
        },
        "id": "1JBA8_Aw6OK_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
        "\n",
        "  # Empty dict for n-grams\n",
        "    n_grams = {}\n",
        "\n",
        "  # Iterate over all sentences in the dataset\n",
        "    for sentence in data:\n",
        "\n",
        "    # Append n start tokens and a single end token to the sentence\n",
        "        sentence = [start_token]*n + sentence + [end_token]\n",
        "\n",
        "    # Convert the sentence into a tuple\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "    # Temp var to store length from start of n-gram to end\n",
        "        m = len(sentence) if n==1 else len(sentence)-1\n",
        "\n",
        "    # Iterate over this length\n",
        "        for i in range(m):\n",
        "\n",
        "      # Get the n-gram\n",
        "            n_gram = sentence[i:i+n]\n",
        "\n",
        "      # Add the count of n-gram as value to our dictionary\n",
        "      # IF n-gram is already present\n",
        "            if n_gram in n_grams.keys():\n",
        "                n_grams[n_gram] += 1\n",
        "      # Add n-gram count\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "\n",
        "    return n_grams"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:27:07.877811Z",
          "iopub.execute_input": "2025-04-29T09:27:07.878293Z",
          "iopub.status.idle": "2025-04-29T09:27:07.888666Z",
          "shell.execute_reply.started": "2025-04-29T09:27:07.878243Z",
          "shell.execute_reply": "2025-04-29T09:27:07.887559Z"
        },
        "id": "UII38gtB6OK_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n",
        "\n",
        "\n",
        "$$\\large\n",
        "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### K-smoothing\n",
        "\n",
        "But what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n",
        "\n",
        "$$\\large\n",
        "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n) + k}{C(w_{n‚àíN+1:n‚àí1} + k |V|)}\n",
        "$$"
      ],
      "metadata": {
        "id": "gybFybZl6OK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
        "\n",
        "  # Convert the previous_n_gram into a tuple\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "  # The Denominator\n",
        "    denom = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "  # previous n-gram plus the current word as a tuple\n",
        "    nplus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero\n",
        "    nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
        "\n",
        "  # Numerator\n",
        "    num = nplus1_gram_count + k\n",
        "\n",
        "  # Final Fraction\n",
        "    prob = num / denom\n",
        "    return prob"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:27:07.8899Z",
          "iopub.execute_input": "2025-04-29T09:27:07.890254Z",
          "iopub.status.idle": "2025-04-29T09:27:07.908402Z",
          "shell.execute_reply.started": "2025-04-29T09:27:07.890222Z",
          "shell.execute_reply": "2025-04-29T09:27:07.907301Z"
        },
        "id": "kf3die356OLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn."
      ],
      "metadata": {
        "id": "u33I3Jc46OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
        "\n",
        "  # Convert to Tuple\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Add end and unknown tokens to the vocabulary\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "\n",
        "  # Calculate the size of the vocabulary\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "  # Empty dict for probabilites\n",
        "    probabilities = {}\n",
        "\n",
        "  # Iterate over words\n",
        "    for word in vocabulary:\n",
        "\n",
        "    # Calculate probability\n",
        "        probability = prob_for_single_word(word, previous_n_gram,\n",
        "                                           n_gram_counts, nplus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "    # Create mapping: word -> probability\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:27:07.910021Z",
          "iopub.execute_input": "2025-04-29T09:27:07.910484Z",
          "iopub.status.idle": "2025-04-29T09:27:07.922337Z",
          "shell.execute_reply.started": "2025-04-29T09:27:07.910433Z",
          "shell.execute_reply": "2025-04-29T09:27:07.9212Z"
        },
        "id": "Mv0ua-Fj6OLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"auto-complete\"></a>\n",
        "# üí¨ The Auto-Complete System"
      ],
      "metadata": {
        "id": "XkBZov9D6OLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability."
      ],
      "metadata": {
        "id": "BshSKiqj6OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    # most recent 'n' words\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    # Calculate probabilty for all words\n",
        "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
        "\n",
        "    # Intialize the suggestion and max probability\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "\n",
        "    # Iterate over all words and probabilites, returning the max.\n",
        "    # We also add a check if the start_with parameter is provided\n",
        "    for word, prob in probabilities.items():\n",
        "\n",
        "        if start_with != None:\n",
        "\n",
        "            if not word.startswith(start_with):\n",
        "                continue\n",
        "\n",
        "        if prob > max_prob:\n",
        "\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:27:07.923634Z",
          "iopub.execute_input": "2025-04-29T09:27:07.92404Z",
          "iopub.status.idle": "2025-04-29T09:27:07.942218Z",
          "shell.execute_reply.started": "2025-04-29T09:27:07.924004Z",
          "shell.execute_reply": "2025-04-29T09:27:07.940904Z"
        },
        "id": "folvEQAP6OLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well."
      ],
      "metadata": {
        "id": "Cu3bm4Wj6OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "    # See how many models we have\n",
        "    count = len(n_gram_counts_list)\n",
        "\n",
        "    # Empty list for suggestions\n",
        "    suggestions = []\n",
        "\n",
        "    # IMP: Earlier \"-1\"\n",
        "\n",
        "    # Loop over counts\n",
        "    for i in range(count-1):\n",
        "\n",
        "        # get n and nplus1 counts\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        # get suggestions\n",
        "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
        "                                    nplus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        # Append to list\n",
        "        suggestions.append(suggestion)\n",
        "\n",
        "    return suggestions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:27:07.943965Z",
          "iopub.execute_input": "2025-04-29T09:27:07.944343Z",
          "iopub.status.idle": "2025-04-29T09:27:07.958059Z",
          "shell.execute_reply.started": "2025-04-29T09:27:07.944289Z",
          "shell.execute_reply": "2025-04-29T09:27:07.956934Z"
        },
        "id": "4ePROB-w6OLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"inference\"></a>\n",
        "# üòä Inference"
      ],
      "metadata": {
        "id": "UFWPiSWU6OLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create a list of n-gram counts for a arbitrary range `(1,6)`"
      ],
      "metadata": {
        "id": "xTKVWEFd6OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    n_model_counts = count_n_grams(final_train, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:27:07.95959Z",
          "iopub.execute_input": "2025-04-29T09:27:07.959981Z",
          "iopub.status.idle": "2025-04-29T09:28:04.695199Z",
          "shell.execute_reply.started": "2025-04-29T09:27:07.959948Z",
          "shell.execute_reply": "2025-04-29T09:28:04.694225Z"
        },
        "id": "3aV-d4zk6OLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0"
      ],
      "metadata": {
        "id": "mNDM_cUb6OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"this\", \"is\", \"my\"]\n",
        "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "display(suggestion)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:28:04.696334Z",
          "iopub.execute_input": "2025-04-29T09:28:04.696657Z",
          "iopub.status.idle": "2025-04-29T09:28:05.405232Z",
          "shell.execute_reply.started": "2025-04-29T09:28:04.696623Z",
          "shell.execute_reply": "2025-04-29T09:28:05.4042Z"
        },
        "id": "lN3sfYvW6OLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "5eb48520-f8df-4846-f24f-504f0dde6465"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('life', 0.030152087889859545),\n",
              " ('life', 0.00091055331289647),\n",
              " ('life', 0.00031171098157788097),\n",
              " ('in', 3.129596594998905e-05)]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"misc\"></a>\n",
        "# üßê Miscellaneous"
      ],
      "metadata": {
        "id": "BQ8SFjDx6OLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how many n-grams we have in our corpus."
      ],
      "metadata": {
        "id": "S2MgQ3fE6OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"unigram count:\" , len(n_gram_counts_list[0]))\n",
        "print(\"bigram count:\", len(n_gram_counts_list[1]))\n",
        "print(\"trigram count:\", len(n_gram_counts_list[2]))\n",
        "print(\"quadgram count:\", len(n_gram_counts_list[3]))\n",
        "print(\"quintgram count:\", len(n_gram_counts_list[4]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:28:05.406612Z",
          "iopub.execute_input": "2025-04-29T09:28:05.406945Z",
          "iopub.status.idle": "2025-04-29T09:28:05.413903Z",
          "shell.execute_reply.started": "2025-04-29T09:28:05.406913Z",
          "shell.execute_reply": "2025-04-29T09:28:05.412883Z"
        },
        "id": "8d2FEPtS6OLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c2a20b-b1e5-479a-a97b-67b56e45dfc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram count: 31954\n",
            "bigram count: 1464002\n",
            "trigram count: 5136867\n",
            "quadgram count: 8357719\n",
            "quintgram count: 9883417\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we just export this list to a `.txt` file so that we can use this for inference rather than \"training\" each time."
      ],
      "metadata": {
        "id": "QKdKafzr6OLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing to file\n",
        "with open(\"en_counts.txt\", 'wb') as f:\n",
        "    pickle.dump(n_gram_counts_list, f)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:28:05.415287Z",
          "iopub.execute_input": "2025-04-29T09:28:05.415667Z",
          "iopub.status.idle": "2025-04-29T09:28:29.778569Z",
          "shell.execute_reply.started": "2025-04-29T09:28:05.415631Z",
          "shell.execute_reply": "2025-04-29T09:28:29.77734Z"
        },
        "id": "Eq3ILHwP6OLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing to file\n",
        "with open(\"vocab.txt\", 'wb') as f:\n",
        "    pickle.dump(vocabulary, f)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-29T09:28:29.780031Z",
          "iopub.execute_input": "2025-04-29T09:28:29.780572Z",
          "iopub.status.idle": "2025-04-29T09:28:29.797344Z",
          "shell.execute_reply.started": "2025-04-29T09:28:29.780527Z",
          "shell.execute_reply": "2025-04-29T09:28:29.796195Z"
        },
        "id": "AEWv4zwa6OLA"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}